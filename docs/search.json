[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Name is Orin Crouse",
    "section": "",
    "text": "Hopefully this helps me get a highly paid position in a very fun career field."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Line Added 11/13/2023\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "hoobies.html",
    "href": "hoobies.html",
    "title": "Hobbies",
    "section": "",
    "text": "I like to snowboard, play with my dogs, and read."
  },
  {
    "objectID": "hoobies.html#in",
    "href": "hoobies.html#in",
    "title": "Political Risk Insurance for Canadian Oil and Gas Companies",
    "section": "In",
    "text": "In"
  },
  {
    "objectID": "hoobies.html#introduction",
    "href": "hoobies.html#introduction",
    "title": "PRI Canadian O&G",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "PRI.html",
    "href": "PRI.html",
    "title": "PRI Canadian O&G",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'patchwork' was built under R version 4.2.3\nCall:\nlm(formula = Y ~ X - 1)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.317e+10 -6.101e+09 -9.271e+08  4.925e+09  1.814e+10 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nX  7658514     856057   8.946 5.98e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.427e+09 on 23 degrees of freedom\nMultiple R-squared:  0.7768,    Adjusted R-squared:  0.7671 \nF-statistic: 80.04 on 1 and 23 DF,  p-value: 5.982e-09\n0%         25%         50%         75%        100% \n    6202000   107093500   632554000  2139100000 11122000000 \n\n\n         0%         25%         50%         75%        100% \n  175309550  1089975255  2226164000  4395065000 22235690000 \n\n\n         0%         25%         50%         75%        100% \n  507370000  5290501800 10277350000 13105740000 38514920000 \n\n\n         0%         25%         50%         75%        100% \n  671600000  9954844500 20411500000 23796500000 49010000000 \n\n\n         0%         25%         50%         75%        100% \n   56363340  1834950578 28238550000 47232915000 74038800000 \n\n\n         0%         25%         50%         75%        100% \n    4353880   375289860  6992851040 22677200000 58035120000 \n\n\n         0%         25%         50%         75%        100% \n    9938807   282860570  5106080000 14789960000 37879180000 \n\n\n         0%         25%         50%         75%        100% \n     144000   351478800  4427627200 23556000000 51481600000 \n\n\n          0%          25%          50%          75%         100% \n   146673668   1200914000   5198732600  45515800000 158000000000\nThis paper explores the concept of Political Risk Insurance (PRI) for Canadian Oil and Gas companies. The examination is to determine whether these companies should seek out PRI with results that hope to show the benefits or consequences. Having the conditions of the country’s influence from the business cycle, rare world changing events, and federal elections. Simulations that have changes in probabilities and random distributions to cover possible outcomes and repeat a thousand times. The results show that the rare usage of PRI is simply a lack of knowledge. Another finding is that from a policy point of view, companies see an ebb and flow with revenue. So, why spend money in attempt to sway the outcome of elections if it is shown to futile."
  },
  {
    "objectID": "PRI.html#introduction",
    "href": "PRI.html#introduction",
    "title": "PRI Canadian O&G",
    "section": "1. Introduction",
    "text": "1. Introduction"
  },
  {
    "objectID": "PRI.html#conceptual-framework",
    "href": "PRI.html#conceptual-framework",
    "title": "PRI Canadian O&G",
    "section": "2. Conceptual Framework",
    "text": "2. Conceptual Framework"
  },
  {
    "objectID": "PRI.html#data",
    "href": "PRI.html#data",
    "title": "PRI Canadian O&G",
    "section": "3. Data",
    "text": "3. Data"
  },
  {
    "objectID": "PRI.html#data-source",
    "href": "PRI.html#data-source",
    "title": "PRI Canadian O&G",
    "section": "3.1 Data Source",
    "text": "3.1 Data Source\nData was primarily observed from The System for Electronic Document Analysis and Retrieval (SEDAR) which is a filing system developed for the Canadian Securities Administrators. Similar to the United States Securities and Exchange Commission (SEC), but is not a federal or province entity, but rather is “is the official site that provides access to most public securities documents and information filed by issuers with the thirteen provincial and territorial securities regulatory authorities (”Canadian Securities Administrators” or “CSA”) in the SEDAR filing system. The statutory objective in making public this filed information is to enhance investor awareness of the business and affairs of issuers and to promote confidence in the transparent operation of capital markets in Canada. Achieving this objective relies heavily on the provision of accurate information on market participants ” (SEDAR, 2022).\nWe used SEDAR as the point of reference to collect oil and gas producing revenue from as many oil and gas companies from Canada that are reported and filled at SEDAR. These fillings go back to 1990, but most data only dates back to 1997 and this is where the discussion starts. Using the median value of revenue from all oil and gas companies from 1997, $2.09 billion (Canadian dollars). This amount is translated with inflation into today’s current dollars. This process was accomplished by using data from the Economic Research in the U.S. Federal Reserve Bank of St. Louis (FRED)\nThe following years were observed to look at elections from 1998 to 2021 from the website directly from the Parliament of Canada. This was to determine which parties were elected to the majority and the probability of those parties. The probabilities were found from various polling firms. From these polls, the data taken was only pertaining to the liberal and conservative parties. The other parties were left out as to they did provide a significant impact (Insert Evidence)\nEach year that falls in-between elections were tracked by the business cycle for Canada, whether the aggregate of each year individually was an expansion year or contraction year. Although the business cycle fluctuates more within a year, the aggregate for the whole year was used as this influences voting more so (Insert Evidence). This data was found from FRED."
  },
  {
    "objectID": "PRI.html#data-processing",
    "href": "PRI.html#data-processing",
    "title": "PRI Canadian O&G",
    "section": "3.2 Data processing",
    "text": "3.2 Data processing\nThe data was pulled, and formatted in order to show an easy representation that can be read easily. The dollar amount of each was translated with the Consumer Price Index (CPI) calculator (from the Bank of Canada) to show each year’s revenue into the dollar amount of 2021. Meaning, given the graph below (Will Reformat tables and graphs to reference better) you can see that 1997’s revenue dollar amount is multiplied by 1.5740 to be the equivalent amount if that was earned in 2021.\n\n\n\n\n\nRevenue earned by all companies together per year. Each year is in today’s dollars (as of 2022).\n\n\n\n\nThe simulation started in the year 1998 where from the polling information it was determined that the liberal party had a 53% chance of winning over the conservative party at 47%. Every year following, whether calculating for the business cycle or for the next election, the probabilities were based on the given condition of the previous year. It follows as such that in 1999 there was no election, so the business cycle was calculated. This was seen has there being a 99% of expansion following a liberal win, there being a 67% chance of contraction following a conservative win. Probability for a contraction year is simply 1 minus the probability of growth. Similarly, the next election was in 2001 and it follows that the probability for a liberal win following an expansion is 75% while following a contraction is 67%. Again it follows that the probability of not winning is simply the 1 minus the probability of winning.\nWith calculations following election years, it is used that a expansion rate of 1.021 and a contraction rate of 1.016. These rates were calculated with the found aggregate for each year, compiled together, and then set for the average rate over the 22 year time span. The rates are used with random normal distribution to predict an amount of revenue that all companies median revenue amount would end in. For the expansion rate, there is a mean of 2.07 and a standard deviation of 0.64. For the contraction rate, there is a mean of 1.63 and a standard deviation of 1.27.\nThe amount for post year is used to determine an insurance amount. This amount is would be the coverage amount that should be sought after in order to protect profit. If the business cycle for the year is expansion, the predicted post amount is multiplied by .95, but if it is a contraction the predicted post amount is multiplied by .60. If there is an expansion following an election, a company should experience growth in revenue, therefore, there should not be a reason for insurance to be used if there is no loss experienced. As for a contraction, a loss in revenue not only could happen but is expected. With the economy of the country going through a downturn, a company for oil and gas would not be able to not experience the same. The hope would be to use an insurance policy to cover any and all loss. We use the value of 60% as a baseline to determine if there is a correlation, and enough to show that there is evidence to a reason for PRI. The reason why the insurance is not more than 60%, is for the insurance company (input more reason given reading articles as to why this is 60 and not closer to 80 or 85) also needs to mitigate loss. The remaining 40% from the predicted amount would be the amount given to an insurance company in premiums and a deductible to use the policy."
  },
  {
    "objectID": "PRI.html#data-results",
    "href": "PRI.html#data-results",
    "title": "PRI Canadian O&G",
    "section": "3.3 Data Results",
    "text": "3.3 Data Results\nThe results from the simulation was interesting to say the least. Different outcomes came from different inputs, as expected, but when there were similar inputs, there were not so similar outputs. ^&. We also see when there are different inputs, there would be similar outputs. For example, the years 1999 and 2021, both followed elections that led to a Liberal win. Now 1999 was an expansion year while 2021 was a contraction. For both years the prediction model under performed greatly from the actual end of year amount was (2021 even more so). How can we explain the differences? This would a be a great question for another analysis, all we can say is that different time periods are less predictable than others.\nAmong the well predicted"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "My Name is Orin Crouse",
    "section": "Skills",
    "text": "Skills\n-Proficient in programs R, Python, and Microsoft Office Suite.\n-Fully capable of preforming independent research.\n-Able to be trained with quick turn around to train others to same high level of standard."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "My Name is Orin Crouse",
    "section": "Education",
    "text": "Education\n-College courses for Mathematics were focused on statistics, regression, and applied programming.\n-College courses for Economics were focused on econometrics, and macroeconomics.\n-Research developed while at the university was a combination of above."
  },
  {
    "objectID": "ChainPulse_ultimate.html#team-members",
    "href": "ChainPulse_ultimate.html#team-members",
    "title": "Decentralization of Cryptocurrency Tokens on the Ethereum Blockchain",
    "section": "Team Members",
    "text": "Team Members\n\nAlex Schaef &gt;Mathematical Analyst. Took the lead in understanding previous projects and linking various variables to the concept of decentralized finance. Also acted as lead notetaker for meetings both within the group and with sponsors from Chainpulse, and has written a majority of the text blocks in this notebook, including “Introduction,” “About the Data,” “Conclusion,” and some graph breakdowns.\nNathaniel Hamilton Thompson &gt; Project Manager. Oversaw management, led in creating the visualization of data, including breaking the data into time segments, and did research to find numerous outside resources to aid in the group’s understanding of the project.\nOrin Crouse &gt; Python Coder. Took the lead in all coding-related aspects of the project. Using the code provided by a previous project, Orin converted it to run successfully with new data to expand on the findings of the previous project and ensured that it continued to run despite new errors that have come with updates to various packages. &gt; Put in the following sections &gt;&gt; “General Approach”, “Code Starts”, “Scatter plot by Months”, “Sorting, Defining Core-Periphery Structure”, “Form Graph”, “Network Dynamics”. Repeated for Dydx Token. Then added “Citiations”."
  },
  {
    "objectID": "ChainPulse_ultimate.html#general-approach",
    "href": "ChainPulse_ultimate.html#general-approach",
    "title": "Decentralization of Cryptocurrency Tokens on the Ethereum Blockchain",
    "section": "General Approach",
    "text": "General Approach\nThis notebook take the approach of taking the networks for the Euler token and reframing the data into simpler, more readable outputs to better explain the idea of the centralization for the tokens. A quote to describe the idea used is  “core-periphery structure in its simplest form refers to a partition of a network into two groups of nodes called core and periphery, where core nodes are densely interconnected (i.e., adjacent), and peripheral nodes are adjacent to the core nodes but not to other peripheral nodes”.[2]\nIf a token is to be described as being a central network we would see our graphs consist of core nodes, while a decentralized network would consist of periphery nodes.\nCore-periphery pairs are defined to have the properties of: \\[A^* = A_{ij}^* = (x_i+x_j - x_ix_j)δ(c_i,c_j) \\] We see \\(x_i=1\\) for core nodes and \\(x_i=0\\) for peripheral nodes. The index of the core-periphery pair to which node \\(i\\) belongs to is represented by \\(c_i(1 ≤ c_i ≤ C)\\). The following properties are treated like axioms here. 1.) Every code node is adjacent to every other code node. 2.) Every core node is adjacent to all corresponding peripheral nodes. 3.) Every peripheral node is not adjacent to any other peripheral node. 4.) Lastly, there are no edges between different idealised core-periphery pairs. [3, 4]  When computing you want \\(c_i, x_i ∈ (1 ≤ i ≤ N)\\) to be maximized comparetively between \\(A\\) and \\(A^*\\). This is shown by:\n\\[Q_{config}^{cp} = \\frac{1}{2M} \\sum_{i=1}^N \\sum_{j=1}^N A_{ij}A_{ij}^* - Ε [\\frac{1}{2M} \\sum_{i=1}^N \\sum_{j=1}^{conf}(1-A_{ij}^*)]\\] \nUsing a configuration model where the epected number of edges between nodes \\(i\\) and \\(j\\) can be represented by \\(Ε[A_{ij}^{conf}]=\\frac{d_id_j}{2M}\\).\nIn the section titled “Construct Continuous Core-Periphery Structures”, various graphs are fromed, each with different algorithms."
  },
  {
    "objectID": "ChainPulse_ultimate.html#about-the-data",
    "href": "ChainPulse_ultimate.html#about-the-data",
    "title": "Decentralization of Cryptocurrency Tokens on the Ethereum Blockchain",
    "section": "About the Data",
    "text": "About the Data\nOur data sets consist of five variables. &gt;There are three “addresses” that consist of a string of numbers and letters that are used to assign a unique moniker to each crypto token and to each account that trades cryptocurrency. They are all categorical data, as they are a name/identifier. The actual numbers and letters are not important, as they are only used to differentiate between different tokens and between different traders.\n\n\nToken_address is always the same within each data set because there is only one token per data set (in this project, we are using two data sets covering the Euler and dYdX tokens).\n\n\n\n\nFrom_address indicates which account is selling the tokens.\n\n\n\n\nTo_address indicates which account is buying the tokens.\n\n\n\nBlock_timestamp tells us the time and day of the transaction.\n\n\nValue shows the number of tokens traded in the transaction (numerical).\n\nOther variables we caluclate include:\n\nGiant Component size ratio: “Giant componenet” is defined as the node with the most edges, or the address with the largest number of transactions for that time period. The ratio is calculated as (# edges on giant component) / (total # of edges)\nRange: (0,1]\n\n\nModularity: Under this calculation, nodes are separated into communities to examine if separate communities exist. The modularity score is the fraction of the edges that fall within the given groups minus the expected fraction if edges were distributed at random. The existance of multiple communities indicate a decentralized network, as transactions are not linked to a single central point.\n\n\nStandard Deviation of Degree Centrality: “Degree Centrality” is equal to the number of edges on a node. This variable is simply the standard deviation of all nodes’ degree centralities in a given time frame.\n\n\n# @title Background Code and Libraries Used\n#%%capture\n!pip install matplotlib==3.5.1 &&gt; /dev/null\n!pip install --upgrade scipy networkx &&gt; /dev/null\n!pip install cpnet &&gt; /dev/null\n\nimport cpnet\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport os\nimport time\nimport zipfile\nfrom tqdm import tqdm\nimport community.community_louvain as community\nfrom numpy import *\nimport random\nimport requests\nimport json\nimport datetime\n\nprint(\"The libraries used are: matplotlib (verison 3.5.1), scipy, networkx, cpnet, numpy, pandas, os, time, zipfile, tqdm,\")\nprint(\"community.community_louvain, random, requests, json, and datetime.\")\n\nThe libraries used are: matplotlib (verison 3.5.1), scipy, networkx, cpnet, numpy, pandas, os, time, zipfile, tqdm,\ncommunity.community_louvain, random, requests, json, and datetime.\n\n\n#Euler Token\n\n# @title Upload Data\n\nurl = 'https://raw.githubusercontent.com/nghthompson/Math_Clinic_Project/main/Euler%20Crypto%20Data.csv'\nedf = pd.read_csv(url)\n\n##Network Visualization\n\n# @title Refinement of Data\n\n#Number Wallet IDs for simplification\n\nunique_addresses = set(edf['from_address'].unique()) | set(edf['to_address'].unique())\nmapping = {address: n for n, address in enumerate(unique_addresses)}\n\nedf[['from_ID', 'to_ID']] = edf[['from_address', 'to_address']].replace(mapping)\n\n# edit timestamp to sort by days\n\nedf.rename(columns={'f0_':'value'}, inplace = True)\nedf = edf.dropna()\nedf['value'] = edf['value'].apply(lambda x: float(x))\nedf['timestamp'] = pd.to_datetime(edf['block_timestamp'])\nedf['timestamp'] = edf['timestamp'].apply(lambda x: str(x)[:10])\nedf['timestamp'] = pd.to_datetime(edf['timestamp'])\n\n# breakdown dataframe into weeks and months\n\nall_time = edf\njune = edf[(edf.timestamp &gt;= '2022-06-01') & (edf.timestamp &lt;= '2022-06-30')]\njuly = edf[(edf.timestamp &gt;= '2022-07-01') & (edf.timestamp &lt;= '2022-07-31')]\naugust = edf[(edf.timestamp &gt;= '2022-08-01') & (edf.timestamp &lt;= '2022-08-31')]\nweek1 = edf[(edf.timestamp &gt;= '2022-06-19') & (edf.timestamp &lt;= '2022-06-25')]\nweek2 = edf[(edf.timestamp &gt;= '2022-06-26') & (edf.timestamp &lt;= '2022-07-02')]\nweek3 = edf[(edf.timestamp &gt;= '2022-07-03') & (edf.timestamp &lt;= '2022-07-09')]\nweek4 = edf[(edf.timestamp &gt;= '2022-07-10') & (edf.timestamp &lt;= '2022-07-16')]\nweek5 = edf[(edf.timestamp &gt;= '2022-07-17') & (edf.timestamp &lt;= '2022-07-23')]\nweek6 = edf[(edf.timestamp &gt;= '2022-07-24') & (edf.timestamp &lt;= '2022-07-30')]\nweek7 = edf[(edf.timestamp &gt;= '2022-07-31') & (edf.timestamp &lt;= '2022-08-06')]\nweek8 = edf[(edf.timestamp &gt;= '2022-08-07') & (edf.timestamp &lt;= '2022-08-13')]\nweek9 = edf[(edf.timestamp &gt;= '2022-08-14') & (edf.timestamp &lt;= '2022-08-20')]\nweek10 = edf[(edf.timestamp &gt;= '2022-08-21') & (edf.timestamp &lt;= '2022-08-27')]\n\nall_time_g = nx.from_pandas_edgelist(edf, source='from_ID', target='to_ID', edge_attr='value')\njune_g = nx.from_pandas_edgelist(june, source='from_ID', target='to_ID', edge_attr='value')\njuly_g = nx.from_pandas_edgelist(july, source='from_ID', target='to_ID', edge_attr='value')\naugust_g = nx.from_pandas_edgelist(august, source='from_ID', target='to_ID', edge_attr='value')\nweek1_g = nx.from_pandas_edgelist(week1, source='from_ID', target='to_ID', edge_attr='value')\nweek2_g = nx.from_pandas_edgelist(week2, source='from_ID', target='to_ID', edge_attr='value')\nweek3_g = nx.from_pandas_edgelist(week3, source='from_ID', target='to_ID', edge_attr='value')\nweek4_g = nx.from_pandas_edgelist(week4, source='from_ID', target='to_ID', edge_attr='value')\nweek5_g = nx.from_pandas_edgelist(week5, source='from_ID', target='to_ID', edge_attr='value')\nweek6_g = nx.from_pandas_edgelist(week6, source='from_ID', target='to_ID', edge_attr='value')\nweek7_g = nx.from_pandas_edgelist(week7, source='from_ID', target='to_ID', edge_attr='value')\nweek8_g = nx.from_pandas_edgelist(week8, source='from_ID', target='to_ID', edge_attr='value')\nweek9_g = nx.from_pandas_edgelist(week9, source='from_ID', target='to_ID', edge_attr='value')\nweek10_g = nx.from_pandas_edgelist(week10, source='from_ID', target='to_ID', edge_attr='value')\n\n\n# @title Network of All Transactions (June - August)\n\nall_comm = nx.community.label_propagation_communities(all_time_g)\ncommunity_index = {n: i for i, com in enumerate(all_comm) for n in com}\n\nplt.figure(figsize=(17, 17))\nplt.title('Network Graph'.format(len(all_time.index)))\nall_color = [community_index[n] for n in all_time_g]\nprint(len(all_time.index), 'Number Transactions for the entire data set')\nnx.draw(all_time_g, with_labels=False, node_size=13, node_color=all_color)\n\n11372 Number Transactions for the entire data set\n\n\n\n\n\n\n# @title Network of Each Month Broke Out\n\njune_comm = nx.community.label_propagation_communities(june_g)\ncommunity_index = {n: i for i, com in enumerate(june_comm) for n in com}\n\nplt.figure(figsize=(17, 17))\nplt.title('{:} Euler Transactions (June 2022)'.format(len(june.index)))\njune_color = [community_index[n] for n in june_g]\nprint(len(june.index), 'Number Transactions for June')\nnx.draw(june_g, with_labels=False, node_size=5, node_color=june_color)\n\n# network of July transactions\n\njuly_comm = nx.community.label_propagation_communities(july_g)\ncommunity_index = {n: i for i, com in enumerate(july_comm) for n in com}\n\nplt.figure(figsize=(17, 17))\nplt.title('Euler Transactions (July 2022)'.format(len(july.index)))\njuly_color = [community_index[n] for n in july_g]\nprint(len(july.index), 'Number Transactions for July')\nnx.draw(july_g, with_labels=False, node_size=5, node_color=july_color)\n\n# network of August transactions\n\naugust_comm = nx.community.label_propagation_communities(august_g)\ncommunity_index = {n: i for i, com in enumerate(august_comm) for n in com}\n\nplt.figure(figsize=(17, 17))\nplt.title('Euler Transactions (August 2022)'.format(len(august.index)))\naugust_color = [community_index[n] for n in august_g]\nprint(len(august.index), 'Number Transactions for August')\nnx.draw(august_g, with_labels=False, node_size=5, node_color=august_color)\n\n5485 Number Transactions for June\n3396 Number Transactions for July\n2490 Number Transactions for August\n\n\n\n\n\n\n\n\n\n\n\n\n# @title Scatter Plot of June. Day by Day\n\nplt.subplot(1,2,1)\nplt.scatter(june['timestamp'], june['to_ID'])\nplt.title('Buyer Transactions (June 2022)'.format(len(june.index)))\n\nplt.subplot(1,2,2)\nplt.scatter(june['from_ID'], june['to_ID'])\nplt.title('Seller Transactions (June 2022)'.format(len(june.index)))\n\nplt.tight_layout()\n\n\n\n\n\n# @title Scatter Plot of July. Day by Day\n\nplt.subplot(1,2,1)\nplt.scatter(july['timestamp'], july['to_ID'])\nplt.title('Buyer Transactions (July 2022)'.format(len(july.index)))\n\nplt.subplot(1,2,2)\nplt.scatter(july['from_ID'], july['to_ID'])\nplt.title('Seller Transactions (July 2022)'.format(len(july.index)))\n\nplt.tight_layout()\n\n\n\n\n\n# @title Scatter Plot of August. Day by Day\n\nplt.subplot(1,2,1)\nplt.scatter(august['timestamp'], august['to_ID'])\nplt.title('Buyer Transactions (August 2022)'.format(len(august.index)))\n\nplt.subplot(1,2,2)\nplt.scatter(august['from_ID'], august['to_ID'])\nplt.title('Seller Transactions (August 2022)'.format(len(august.index)))\n\nplt.tight_layout()"
  },
  {
    "objectID": "ChainPulse_ultimate.html#formation-of-core-periphery-graphs",
    "href": "ChainPulse_ultimate.html#formation-of-core-periphery-graphs",
    "title": "Decentralization of Cryptocurrency Tokens on the Ethereum Blockchain",
    "section": "Formation of Core-Periphery Graphs",
    "text": "Formation of Core-Periphery Graphs\n\n# @title Sorting and Defining Core-Periphery Structure\n\nedf.sort_values('block_timestamp', ascending = True)\n\n# data cleaning\n\nedf = edf[edf['timestamp']&gt;='2021-12-30']\nedf = edf[edf['timestamp']&lt;='2022-08-24']\n\n\nedf.to_csv('Euler Raw Transfer Data.csv')\n\nedf = edf.drop(columns = ['token_address','block_timestamp'])\n\n## add values between the 2 same addresses together\nedf[['from_address', 'to_address']] = np.sort(edf[['from_address', 'to_address']], axis=1)\nedf= edf.groupby(['timestamp','from_address','to_address']).agg(lambda x: sum(x)).reset_index()\nedf.to_csv('Euler transaction data_after preprocessing.csv')\nedf = pd.read_csv('Euler transaction data_after preprocessing.csv')\n\n##Network Analysis\n\nedf_time_partition= edf.groupby(['timestamp'])['to_address'].agg(['nunique']).reset_index()\nedf_time_partition = edf_time_partition.drop(['nunique'], axis=1)\n\n##Number of daily edges\n\nnum_nodes = []\nnum_edges = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # MultiDi Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Number of nodes, number of edges\n    nodes = G.number_of_nodes()\n    edges = G.number_of_edges()\n    num_nodes.append(nodes)\n    num_edges.append(edges)\n\n    Network_Features={\"num_nodes\" : num_nodes,\"num_edges\" : num_edges}\nNetwork_Features=pd.DataFrame(Network_Features)\nNetwork_Features['time'] =  edf_time_partition['timestamp']\n\n### Degree Setting\n\nDegreemean = []\nDegreestd = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # MultiDi Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Degree_centrality, mean_value\n    degrees = G.degree()\n    degree = list(dict(G.degree()).values())\n    edf_deg = {\"Degree\" : degree}\n    edf_deg = pd.DataFrame(edf_deg)\n    DC_mean = edf_deg['Degree'].mean()\n    DC_std = edf_deg['Degree'].std()\n    Degreemean.append(DC_mean)\n    Degreestd.append(DC_std)\n\nNetwork_Features['Degree mean']  = Degreemean\nNetwork_Features['Degree std']  = Degreestd\n\n##Extract Top 10 by ratio\n\ntop10Degreemean = []\ntop10Degreestd = []\n\nfor i in range(0,len(edf_time_partition)):\n\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    sender_mdegree= edf_1.groupby(['from_address'])['to_address'].count().reset_index()\n    receiver_mdegree = edf_1.groupby(['to_address'])['from_address'].count().reset_index()\n    sender_mdegree = sender_mdegree.rename(columns={'to_address':'degree'})\n    sender_mdegree = sender_mdegree.rename(columns={'from_address':'address'})\n    receiver_mdegree = receiver_mdegree.rename(columns = {'from_address':'degree'})\n    receiver_mdegree = receiver_mdegree.rename(columns = {'to_address':'address'})\n\n    merge = pd.merge(sender_mdegree,receiver_mdegree,on=\"address\",how = \"outer\")\n    merge = merge.fillna(int(0))\n    merge['degree'] = merge['degree_x']+merge['degree_y']\n\n    merge.sort_values(by=['degree'], ascending=False, inplace=True)\n    merge = merge.reset_index()\n    top5degree = merge['address'][0:10].tolist()\n\n    sen_top =  edf_1[edf_1['from_address'].isin(top5degree)]\n    rec_top= edf_1[edf_1['to_address'].isin(top5degree)]\n\n    topaddress = pd.concat([sen_top,rec_top]).drop_duplicates()\n\n    G = nx.from_pandas_edgelist(topaddress, 'from_address', 'to_address', 'value', nx.Graph())\n    # Calculation of absolute degree\n    degree = []\n    for j in range (0,2): ## MUST CHANGE BACK TO (0,10) LATER. FIND OUT WHY top5degree IS ONLY THREE VARIABLES\n        degrees = G.degree(top5degree[j])\n        degree.append(degrees)\n    edf_deg = {\"Degree\" : degree}\n    edf_deg = pd.DataFrame(edf_deg)\n    deg_mean = edf_deg['Degree'].mean()\n    deg_std = edf_deg['Degree'].std()\n    top10Degreemean.append(deg_mean)\n    top10Degreestd.append(deg_std)\n\nNetwork_Features['Top10Degree mean']  = top10Degreemean\nNetwork_Features['Top10Degree std']  = top10Degreestd\n\nNetwork_Features['Top10 Degree mean ratio']  = Network_Features['Top10Degree mean']/Network_Features['Degree mean']\n\n#The above code cells don't show much being there aren't 10 degrees found, but rather only two.\n#That means there are only two core nodes that reach the criteria to be considered core.\n#Already we see the Euler token to be more decentralized rather than central.\n\n### Degree centrality\nDCmean = []\nDCstd = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # MultiDi Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Degree_centrality, mean_value\n    deg_cen = nx.degree_centrality(G)\n    edf_deg = pd.DataFrame.from_dict(deg_cen, orient='index', columns=['Degree_Centrality'])\n    DC_mean = edf_deg['Degree_Centrality'].mean()\n    DC_std = edf_deg['Degree_Centrality'].std()\n    DCmean.append(DC_mean)\n    DCstd.append(DC_std)\n\n#The above code cell redefines the core nodes found to double check and ensure the understanding of core is reached for the entire network.\n\n### Coefficient Clusters\n\nclustermean = []\nclusterstd = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # Unweighted-Directed Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Clustering_Coefficient, mean_value, std\n    clustering = nx.clustering(G)\n    df_cluster = pd.DataFrame.from_dict(clustering, orient='index', columns=['Clustering_Coefficient'])\n    cluster_mean = df_cluster['Clustering_Coefficient'].mean()\n    cluster_std = df_cluster['Clustering_Coefficient'].std()\n    clustermean.append(cluster_mean)\n    clusterstd.append(cluster_std)\n\n#The above code cell is form the cluster by coefficient. This is for both core and periphery\n\n### Build Modularity\n\nmod_list = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # unweighted-undirected Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of modularity\n    part = community.best_partition(G)\n    mod = community.modularity(part,G)\n    mod_list.append(mod)\n\n    ### Build Transitivity\n\n    tran_list = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # Unweighted-undirected Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of transitivity,\n    tran = nx.transitivity(G)\n    tran_list.append(tran)\n\n### Centrality by Eigenvector\n\neigmean = []\neigstd = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    #edf_1 = actsenrec.loc[actsenrec['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # MultiDi Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Closeness_centrality, mean_value\n    eig_cen = nx.eigenvector_centrality(G, max_iter=20000)\n    edf_eig = pd.DataFrame.from_dict(eig_cen, orient='index', columns=['eigenvector_centrality'])\n    eig_mean = edf_eig['eigenvector_centrality'].mean()\n    eig_std = edf_eig['eigenvector_centrality'].std()\n    eigmean.append(eig_mean)\n    eigstd.append(eig_std)\n\n### Closeness Centrality\n\nCCmean = []\nCCstd = []\nfor i in range(0,len(edf_time_partition)):\n\n    # Data Partition\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n\n    # MultiDi Network Building (weighted-directed graph)\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of Closeness_centrality, mean_value\n    close_cen = nx.closeness_centrality(G)\n    edf_close = pd.DataFrame.from_dict(close_cen, orient='index', columns=['Closeness_Centrality'])\n    CC_mean = edf_close['Closeness_Centrality'].mean()\n    CC_std = edf_close['Closeness_Centrality'].std()\n    CCmean.append(CC_mean)\n    CCstd.append(CC_std)\n\n### Number of components\n\ncomponents_cnt = []\nfor i in range(0,len(edf_time_partition)):\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n    com_cnt = nx.number_connected_components(G)\n    components_cnt.append(com_cnt)\n\n### Gaint component by size and number of nodes\n\ngiant_com_ratio = []\nfor i in range(0,len(edf_time_partition)):\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n# G = nx.Graph()\n    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n    G0 = G.subgraph(Gcc[0])\n#com_cnt = nx.number_connected_components(G)\n#components_cnt.append(com_cnt)\n    nodes = G0.number_of_nodes()\n    nodes_whole = G.number_of_nodes()\n    ratio = nodes/nodes_whole\n    giant_com_ratio.append(ratio)\n\n### Save features output\n\nNetwork_Features['DCmean']=DCmean\nNetwork_Features['DCstd']=DCstd\nNetwork_Features['clustermean']=clustermean\nNetwork_Features['clusterstd']=clusterstd\nNetwork_Features['modularity']=mod_list\nNetwork_Features['transitivity']=tran_list\nNetwork_Features['eig_mean']=eigmean\nNetwork_Features['eig_std']=eigstd\nNetwork_Features['closenessmean']=CCmean\nNetwork_Features['closenessstd']=CCstd\nNetwork_Features['Components_cnt']=components_cnt\nNetwork_Features['giant_com_ratio']=giant_com_ratio\n\nNetwork_Features['token'] =  'Euler'\n#Network_Features\n\nNetwork_Features.to_csv('Euler_Network_Features.csv')\n\n\nConstruct continuous core-periphery structure\n\n# @title Borgatti-Everett Algorithm\n\nedf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][63]]\nG = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n\nalg = cpnet.BE()\nalg.detect(G)\nc = alg.get_pair_id()\nx = alg.get_coreness()  # Get the coreness of nodes\n\n##coreness = pd.DataFrame.from_dict(x, orient='index', columns=['Coreness'])\n##corenessmean = coreness['Coreness'].mean()\n#corenessmean\n\n#sig_c, sig_x, significant, p_values = cpnet.qstest(\n#    c, x, G, alg, significance_level=0.05, num_of_rand_net=100, num_of_thread=16)\n\n#print(significant)\n#print(p_values)\n\npos = nx.spiral_layout(G,scale = 1)\nfig = plt.figure(figsize=(8, 6))\nax = plt.gca()\nax, pos = cpnet.draw(G, c, x, ax, pos = pos)\n\nedf_2 = edf_1[['from_address', 'to_address', 'value']]\n#edf_2.tail()\nresult = edf_2.dtypes\n#print(result)\nseller = edf_2.from_address.unique()\nbuyer = edf_2.to_address.unique()\n\n\n\n\n\n# @title Kojaku-Masuda Algrithm basic\n\nkmconfig = cpnet.KM_config()\nkmconfig.detect(G)\n\nc = kmconfig.get_pair_id()\nx = kmconfig.get_coreness()\n\nfig = plt.figure(figsize=(8, 6))\nax = plt.gca()\nax, _ = cpnet.draw(G, c, x, ax, pos=pos)\n\n\n\n\n\n# @title Kojaku-Masuda Algorithm Search for Short Comings\n\nedf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][63]] #64 was 90. Change later\nG = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of mean&std of coreness (continuous structure)\nalg = cpnet.KM_ER()\nalg.detect(G)\nc = alg.get_pair_id()\nx = alg.get_coreness()  # Get the coreness of nodes\n\n#coreness = pd.DataFrame.from_dict(x, orient='index', columns=['Coreness'])\n#corenessmean = coreness['Coreness'].mean()\n#corenessstd = coreness['Coreness'].std()\n#print ('mean', corenessmean)\n#print ('std', corenessstd)\n\nsig_c, sig_x, significant, p_values = cpnet.qstest(\n    c, x, G, alg, significance_level=0.05, num_of_rand_net=100, num_of_thread=20)\n\npos = nx.spiral_layout(G,scale = 1)\nfig = plt.figure(figsize=(10, 10))\nax = plt.gca()\ndraw_nodes_kwd = {\"node_size\": 80, \"linewidths\": 0.8}\nax, pos = cpnet.utils.draw(G, sig_c, sig_x, ax, draw_nodes_kwd=draw_nodes_kwd)\n\n/usr/local/lib/python3.10/dist-packages/cpnet/qstest.py:94: UserWarning: 'num_of_thread keyword' is duplicated due to a compatibility issue with numba. Only one CPU will be used.\n  warnings.warn(\"'num_of_thread keyword' is duplicated due to a compatibility issue with numba. Only one CPU will be used.\")\n100%|██████████| 100/100 [00:00&lt;00:00, 191.44it/s]\n\n\n\n\n\n\n# @title Kojaku-Masuda Algorithm with lower significance level\n\nsig_c, sig_x, significant, p_values = cpnet.qstest(\n    c, x, G, kmconfig, significance_level=0.01, num_of_thread=1)\n\n# Visualization\nfig = plt.figure(figsize=(8, 8))\nax = plt.gca()\nax, pos = cpnet.draw(G, sig_c, sig_x, ax, pos=pos)\n\n/usr/local/lib/python3.10/dist-packages/cpnet/qstest.py:94: UserWarning: 'num_of_thread keyword' is duplicated due to a compatibility issue with numba. Only one CPU will be used.\n  warnings.warn(\"'num_of_thread keyword' is duplicated due to a compatibility issue with numba. Only one CPU will be used.\")\n100%|██████████| 100/100 [00:00&lt;00:00, 164.93it/s]\n\n\n\n\n\n\n# @title Minres Algorithm\n\nedf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][63]] #64 was 90. Change later\nG = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of mean&std of coreness (continuous structure)\nalg = cpnet.MINRES()\nalg.detect(G)\nc = alg.get_pair_id()\nx = alg.get_coreness()  # Get the coreness of nodes\n\n#coreness = pd.DataFrame.from_dict(x, orient='index', columns=['Coreness'])\n#corenessmean = coreness['Coreness'].mean()\n#corenessstd = coreness['Coreness'].std()\n#print ('mean', corenessmean)\n#print ('std', corenessstd)\n\nsig_c, sig_x, significant, p_values = cpnet.qstest(\n    c, x, G, alg, significance_level=0.05, num_of_rand_net=100) #, num_of_thread=20\n\n#print(significant)\n#print(p_values)\n#print(sig_c)\n#print(sig_x)\n\nx = alg.get_coreness()  # Get the coreness of nodes\nc = alg.get_pair_id()  # Get the group membership of nodes\n\npos = nx.spiral_layout(G,scale = 1)\nfig = plt.figure(figsize=(10, 10))\nax = plt.gca()\ndraw_nodes_kwd = {\"node_size\": 80, \"linewidths\": 0.8}\nax, pos = cpnet.utils.draw(G, sig_c, sig_x, ax, draw_nodes_kwd=draw_nodes_kwd)\n\n100%|██████████| 100/100 [00:02&lt;00:00, 41.14it/s]\n\n\n\n\n\n\n# @title Surprise Algorithm\n\nedf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][63]] #64 was 90. Change later\nG = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of mean&std of coreness (continuous structure)\nalg = cpnet.Surprise()\nalg.detect(G)\nc = alg.get_pair_id()\nx = alg.get_coreness()  # Get the coreness of nodes\n\n#coreness = pd.DataFrame.from_dict(x, orient='index', columns=['Coreness'])\n#corenessmean = coreness['Coreness'].mean()\n#corenessstd = coreness['Coreness'].std()\n#print ('mean', corenessmean)\n#print ('std', corenessstd)\n\nsig_c, sig_x, significant, p_values = cpnet.qstest(\n    c, x, G, alg, significance_level=0.05, num_of_rand_net=100) #, num_of_thread=20\n\npos = nx.spiral_layout(G,scale = 1)\nfig = plt.figure(figsize=(10, 10))\nax = plt.gca()\ndraw_nodes_kwd = {\"node_size\": 80, \"linewidths\": 0.8}\nax, pos = cpnet.utils.draw(G, sig_c, sig_x, ax, draw_nodes_kwd=draw_nodes_kwd)\n\n100%|██████████| 100/100 [00:00&lt;00:00, 105.10it/s]\n\n\n\n\n\n\n# @title Rossa Algorithm\n\nedf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][63]] #64 was 90. Change later\nG = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n\n    # Calculation of mean&std of coreness (continuous structure)\nalg = cpnet.Rossa()\nalg.detect(G)\nc = alg.get_pair_id()\nx = alg.get_coreness()  # Get the coreness of nodes\n\n#coreness = pd.DataFrame.from_dict(x, orient='index', columns=['Coreness'])\n#corenessmean = coreness['Coreness'].mean()\n#corenessstd = coreness['Coreness'].std()\n#print ('mean', corenessmean)\n#print ('std', corenessstd)\n\nsig_c, sig_x, significant, p_values = cpnet.qstest(\n    c, x, G, alg, significance_level=0.05, num_of_rand_net=100)  #, num_of_thread=20\n\npos = nx.spiral_layout(G,scale = 1)\nfig = plt.figure(figsize=(10, 10))\nax = plt.gca()\ndraw_nodes_kwd = {\"node_size\": 80, \"linewidths\": 0.8}\nax, pos = cpnet.utils.draw(G, sig_c, sig_x, ax, draw_nodes_kwd=draw_nodes_kwd)\n\n100%|██████████| 100/100 [00:00&lt;00:00, 124.33it/s]\n\n\n\n\n\nThese algorithms are meant to show with proof that we can describe core-periphery structure networks. More cores discorved would leads us to believe there is more a centeraliztion finance for a crypto currency token. More periphery nodes with less cores would lead us to believe the token is more decenteralized.\n##Creation of Tablature for Centralization\n\n# @title Conform Core Addresses by Date Counts\n\ncore_address = []\na = 0\nfor i in range(0,len(edf_time_partition)):\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n    alg = cpnet.BE()\n    alg.detect(G)\n    c = alg.get_pair_id()\n    x = alg.get_coreness()\n\n    coredf = pd.DataFrame.from_dict(x, orient='index',columns=['coreness'])\n    core = coredf[coredf['coreness']==1].index.tolist()\n    core_address.extend(core)\n    a+=1\n    #print(a)\n\ncores = pd.DataFrame(core_address)\ncore_cnt = cores[0].value_counts(ascending=False).reset_index()\n#core_cnt\n\ncore_cnt.to_csv('core_date_cnt.csv')\n\n\n# @title Number of Core Members Each Day\n\ncore_cnt = []\nfor i in range(0,len(edf_time_partition)):\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][i]]\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n    alg = cpnet.BE()\n    alg.detect(G)\n    c = alg.get_pair_id()\n    x = alg.get_coreness()\n\n    coredf = pd.DataFrame.from_dict(x, orient='index',columns=['coreness'])\n    core = coredf[coredf['coreness']==1].index.tolist()\n    cnt = len(core)\n    core_cnt.append(cnt)\n\nprint(\"Minimum for a day is :\",np.min(core_cnt))\nprint(\"Maximum for a day is :\",np.max(core_cnt))\nprint(\"Total for all days is :\",np.sum(core_cnt))\n\nMinimum for a day is : 0\nMaximum for a day is : 12\nTotal for all days is : 183\n\n\n\n# @title Average Number of Neighbors of Cores\n\navg_core_neighbor = []\n\nfor i in range(0,len(edf_time_partition)):\n    edf_1 = edf.loc[edf['timestamp']==edf_time_partition['timestamp'][0]]\n    G = nx.from_pandas_edgelist(edf_1, 'from_address', 'to_address', 'value', nx.Graph())\n    alg = cpnet.BE()\n    alg.detect(G)\n    c = alg.get_pair_id()\n    x = alg.get_coreness()\n\ncoredf = pd.DataFrame.from_dict(x, orient='index',columns=['coreness'])\ncore = coredf[coredf['coreness']==1].index.tolist()\n\nneighbor_cnt = []\nfor i in range (0,len(core)):\n    neighbor = G.degree(core[i])\n    neighbor_cnt.append(neighbor)\n\nneighbor_cnt_mean = mean(neighbor_cnt)\navg_core_neighbor.append(neighbor_cnt_mean)\n\nprint(\"Average Neighbors to Cores\", np.sum(neighbor_cnt_mean))\n\nAverage Neighbors to Cores 1.0\n\n\n\n# @title Network Dynamics\n\nedf_2 = pd.read_csv('Euler_Network_Features.csv')\n\nfig,axes = plt.subplots(3,2)\nplt.style.use('default')\n#plt.style.use('seaborn-pastel')\nax = edf_2[['Components_cnt']].plot(ax = axes[0,0], figsize=(13,10), grid=False, title='Number of components',xlabel=' ')\nax.set_yscale('log')\nedf_2[['giant_com_ratio']].plot(ax = axes[0,1], figsize=(13,7), grid=False, title='Giant component size ratio',xlabel=' ')\nedf_2[['modularity']].plot(ax = axes[1,0], figsize=(13,7), grid=False, title='Modularity',xlabel=' ')\n#edf_2[['relative_degree']].plot(ax = axes[1,0], figsize=(13,10), grid=False, title='Relative degree',xlabel=' ')\nedf_2[['DCstd']].plot(ax = axes[1,1], figsize=(13,7), grid=False, title='Std of Degree Centrality',xlabel=' ')\n#edf_2[['cp_test_pvalue']].plot(ax = axes[2,0], figsize=(13,11), grid=False, title='p-value of cp test',xlabel=' ')\n#edf_2[['core_cnt']].plot(ax = axes[2,1], figsize=(13,11), grid=False, title='number of cores',xlabel=' ')\nedf_2[['Top10 Degree mean ratio']].plot(ax = axes[2,0], figsize=(13,10), grid=False, title='Top10 nodes avg degree / general avg degree',xlabel=' ')\n#edf_2[['giant_com_ratio']].plot(ax = axes[2,1], figsize=(13,10), grid=False, title='Giant component size ratio',xlabel=' ')\n\nplt.subplots_adjust(wspace =0.15, hspace =0.35)\n\n\n\n\n\nBreakdown\nNumber of Components: We expect a smaller number of componenets to indicate a more centralized network. The number of components is fairly random with an exception toward the beginning of the recorded data.\nGiant Component Size Ratio: A larger giant component size ratio indicates a more centralized network. In our data, the giant component ratio starts high (suggesting more centralization early on), then becomes random, similar to the number of components.\nModulatiry: A smaller modularity score suggests a more centralized network. Modularity for the Ether token starts low, but quickly rises, and stays fairly stable past day 15. This indicates a more centralized network in early days that becomes decentralized and stays decentralized after a couple of weeks. However, while the modularity for Euler settles around 0.6, the modularity for the AAVE token (found in the original paper) settles around 0.8 before dropping down to 0.7, a possible indication that AAVE is more decentralized than Euler.\nStandard Deviation of Degree Centrality: A higher standard deviation suggests a more centralized network. Standard deviation for Euler starts high but quickly falls by day five, where is stays steady. Like the modularity scores, this suggests a more centralized network early on that quickly becomes and stays decentralized."
  },
  {
    "objectID": "ChainPulse.html",
    "href": "ChainPulse.html",
    "title": "Chain Pulse",
    "section": "",
    "text": "Decentralization of the Cryptocurrency  dy/dx on the Ethereum Blockchain\n\n\nAlex Schaef, Nathaniel Hamilton Thompson, Orin Crouse\n\n\nAbstract\n\nCryptocurrency exchanges heavily emphasize being a “decentralized finance,” in that all transactions of cryptocurrencies are made between two individuals without requiring a central intermediary. However, recent studies have expressed doubt about the decentralized nature of cryptocurrency. In this paper, we study the Euler crypto token (in addition to a previous study done over the AAVE token) to conclude that, although there are some centralized components within the network of transactions, trends across multiple tokens still indicate a decentralized nature to the network.\n#Introduction\nCryptocurrency prides itself on being a truly decentralized finance, meaning transactions can happen between any two individuals, or more, without the need of a central intermediary. In our lives, the best example of an intermediary is a bank. With the U.S. dollar, transactions are normally approved by a bank, meaning most exchanges of money go through a central point; the majority of transactions are between a person and a bank. Cryptocurrency is different in that transactions are made entirely between individuals, with no central intermediary. These transactions are recorded on a blockchain, which is essentially a virtual ledger. Our project is inspired by a previous paper written by Ziqiao Ao, Gergely Horvath, and Luyao Zhang, titled Are Decentralized Finance Really Decentralized? A Social Network Analysis of the AAVE Protocol on the Ethereum Blockchain The paper studies the AAVE token, and attempts to argue that cryptocurrency is not fully decentralized as it claims to be. In this project, we will expand on the studies done in this original paper, with the following goals:\n\n\nConfirm or deny the results of the original paper.\n\n\nUse data from dy/dx token to expand on the results of the original paper.\n\n\nExamine trends between tokens to see if the decentralization of transactions is comparable across different currencies.\n\n\nExamine trends across time to see if different tokens follow a similar trend over the lifetime of the token.\n\n#General Approach\nThis notebook take the approach of taking the networks for the Euler token and re-framing the data into simpler, more readable outputs to better explain the idea of the centralization for the tokens. A quote to describe the idea used is\n“core-periphery structure in its simplest form refers to a partition of a network into two groups of nodes called core and periphery, where core nodes are densely interconnected (i.e., adjacent), and peripheral nodes are adjacent to the core nodes but not to other peripheral nodes”.[2]\nIf a token is to be described as being a central network we would see our graphs consist of core nodes, while a decentralized network would consist of periphery nodes.\nCore-periphery pairs are defined to have the properties of:\n\\[\nA^*=A^*_{ij}=(x_i+x_j-x_ix_j)\\delta(c_i,c_j)\n\\] We see \\(x_i=1\\) for core nodes and \\(x_i=0\\) for peripheral nodes. The index of the core-peripheral pair to which node \\(i\\) belongs to is represented by \\(c_i(1 \\leq c_i \\leq C)\\). The following properties are treated like axioms here.\n\n\nEvery core node is adjacent to every other core node.\n\n\nEvery core node is adjacent to all corresponding peripheral nodes.\n\n\nEvery peripheral node is not adjacent to any other peripheral node.\n\n\nLastly, there are no edges between different idealized core-periphery pairs. \\([3, 4]\\)\n\n\nWhen computing you want \\(c_i,x_i \\in(1 \\leq i \\leq N)\\) not be maximized comparatively between \\(A\\) and \\(A*\\). This is shown by: \\[Q^{cp}_{config} = \\frac{1}{2M} \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}A_{ij}^* - Ε [\\frac{1}{2M} \\sum_{i=1}^N \\sum_{j=1}^{conf}(1-A_{ij}^*)]\\] Using a configuration model where the expected number of edges between nodes \\(i\\) and \\(j\\) can be represented by \\(Ε[A_{ij}^{conf}]=\\frac{d_id_j}{2M}\\).\nIn the section titled “Construct Continuous Core-Periphery Structures”, various graphs are formed, each with different algorithms.\n\n\nCode\n#!pip install matplotlib==3.5.1 &&gt; /dev/null\n#!pip install --upgrade scipy networkx &&gt; /dev/null\n#!pip install cpnet &&gt; /dev/null\n\nimport cpnet\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport os\nimport time\nimport zipfile\nfrom tqdm import tqdm\nimport community.community_louvain as community\nfrom numpy import *\nimport random\nimport requests\nimport json\nimport datetime\n\nprint(\"The libraries used are: matplotlib (verison 3.5.1), scipy, networkx, cpnet, numpy, pandas, os, time, zipfile, tqdm,\")\nprint(\"community.community_louvain, random, requests, json, and datetime.\")\n\n\nurl = 'https://raw.githubusercontent.com/nghthompson/Math_Clinic_Project/main/Euler%20Crypto%20Data.csv'\nedf = pd.read_csv(url)\n# @title Refinement of Data\n\n#Number Wallet IDs for simplification\n\nunique_addresses = set(edf['from_address'].unique()) | set(edf['to_address'].unique())\nmapping = {address: n for n, address in enumerate(unique_addresses)}\n\nedf[['from_ID', 'to_ID']] = edf[['from_address', 'to_address']].replace(mapping)\n\n# edit timestamp to sort by days\n\nedf.rename(columns={'f0_':'value'}, inplace = True)\nedf = edf.dropna()\nedf['value'] = edf['value'].apply(lambda x: float(x))\nedf['timestamp'] = pd.to_datetime(edf['block_timestamp'])\nedf['timestamp'] = edf['timestamp'].apply(lambda x: str(x)[:10])\nedf['timestamp'] = pd.to_datetime(edf['timestamp'])\n\n# breakdown dataframe into weeks and months\n\nall_time = edf\njune = edf[(edf.timestamp &gt;= '2022-06-01') & (edf.timestamp &lt;= '2022-06-30')]\njuly = edf[(edf.timestamp &gt;= '2022-07-01') & (edf.timestamp &lt;= '2022-07-31')]\naugust = edf[(edf.timestamp &gt;= '2022-08-01') & (edf.timestamp &lt;= '2022-08-31')]\nweek1 = edf[(edf.timestamp &gt;= '2022-06-19') & (edf.timestamp &lt;= '2022-06-25')]\nweek2 = edf[(edf.timestamp &gt;= '2022-06-26') & (edf.timestamp &lt;= '2022-07-02')]\nweek3 = edf[(edf.timestamp &gt;= '2022-07-03') & (edf.timestamp &lt;= '2022-07-09')]\nweek4 = edf[(edf.timestamp &gt;= '2022-07-10') & (edf.timestamp &lt;= '2022-07-16')]\nweek5 = edf[(edf.timestamp &gt;= '2022-07-17') & (edf.timestamp &lt;= '2022-07-23')]\nweek6 = edf[(edf.timestamp &gt;= '2022-07-24') & (edf.timestamp &lt;= '2022-07-30')]\nweek7 = edf[(edf.timestamp &gt;= '2022-07-31') & (edf.timestamp &lt;= '2022-08-06')]\nweek8 = edf[(edf.timestamp &gt;= '2022-08-07') & (edf.timestamp &lt;= '2022-08-13')]\nweek9 = edf[(edf.timestamp &gt;= '2022-08-14') & (edf.timestamp &lt;= '2022-08-20')]\nweek10 = edf[(edf.timestamp &gt;= '2022-08-21') & (edf.timestamp &lt;= '2022-08-27')]\n\nall_time_g = nx.from_pandas_edgelist(edf, source='from_ID', target='to_ID', edge_attr='value')\njune_g = nx.from_pandas_edgelist(june, source='from_ID', target='to_ID', edge_attr='value')\njuly_g = nx.from_pandas_edgelist(july, source='from_ID', target='to_ID', edge_attr='value')\naugust_g = nx.from_pandas_edgelist(august, source='from_ID', target='to_ID', edge_attr='value')\nweek1_g = nx.from_pandas_edgelist(week1, source='from_ID', target='to_ID', edge_attr='value')\nweek2_g = nx.from_pandas_edgelist(week2, source='from_ID', target='to_ID', edge_attr='value')\nweek3_g = nx.from_pandas_edgelist(week3, source='from_ID', target='to_ID', edge_attr='value')\nweek4_g = nx.from_pandas_edgelist(week4, source='from_ID', target='to_ID', edge_attr='value')\nweek5_g = nx.from_pandas_edgelist(week5, source='from_ID', target='to_ID', edge_attr='value')\nweek6_g = nx.from_pandas_edgelist(week6, source='from_ID', target='to_ID', edge_attr='value')\nweek7_g = nx.from_pandas_edgelist(week7, source='from_ID', target='to_ID', edge_attr='value')\nweek8_g = nx.from_pandas_edgelist(week8, source='from_ID', target='to_ID', edge_attr='value')\nweek9_g = nx.from_pandas_edgelist(week9, source='from_ID', target='to_ID', edge_attr='value')\nweek10_g = nx.from_pandas_edgelist(week10, source='from_ID', target='to_ID', edge_attr='value')\n\n\nThe libraries used are: matplotlib (verison 3.5.1), scipy, networkx, cpnet, numpy, pandas, os, time, zipfile, tqdm,\ncommunity.community_louvain, random, requests, json, and datetime.\n\n\n\n\nCode\nall_comm = nx.community.label_propagation_communities(all_time_g)\ncommunity_index = {n: i for i, com in enumerate(all_comm) for n in com}\n\nplt.figure(figsize=(10, 10))\nplt.title('Network Graph'.format(len(all_time.index)))\nall_color = [community_index[n] for n in all_time_g]\nprint(len(all_time.index), 'Number Transactions for the entire data set')\nnx.draw(all_time_g, with_labels=False, node_size=13, node_color=all_color)\n\n\n11372 Number Transactions for the entire data set\n\n\n\n\n\n\n\nCode\njune_comm = nx.community.label_propagation_communities(june_g)\ncommunity_index = {n: i for i, com in enumerate(june_comm) for n in com}\n\nplt.figure(figsize=(110 , 10))\nplt.title('{:} Euler Transactions (June 2022)'.format(len(june.index)))\njune_color = [community_index[n] for n in june_g]\nprint(len(june.index), 'Number Transactions for June')\nnx.draw(june_g, with_labels=False, node_size=5, node_color=june_color)\n\n\n5485 Number Transactions for June"
  }
]